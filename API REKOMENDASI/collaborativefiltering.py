# -*- coding: utf-8 -*-
"""collaborativeFiltering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FncAYzAguppHoRWaHTQcLXo061Z-3FT4

# Sistem Rekomendasi anime dengan menggunakan Metode Collaborative Filtering

## Penyiapan Data

### Import Library
"""


from google.colab import files
import os
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import optuna
from pymongo import MongoClient

"""## Data Understanding"""

# Ubah 'your_connection_string' dengan string koneksi MongoDB Anda
client = MongoClient('mongodb+srv://ricardodirkanderson:rikupang@cluster0.x3m6qzb.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')

# Pilih database dan koleksi
db = client['DavDatabase']
collection = db['anime_ratings']

# Ambil data dari koleksi MongoDB
data_from_db = list(collection.find())

# Konversi data ke DataFrame pandas
df = pd.DataFrame(data_from_db)

# Simpan DataFrame sebagai file CSV
df.to_csv('anime_ratings.csv', index=False)

# Informasi pengguna bahwa data telah disimpan
print("Data telah disimpan sebagai 'anime_rating.csv'")

# Ubah 'your_connection_string' dengan string koneksi MongoDB Anda
client = MongoClient('mongodb+srv://ricardodirkanderson:rikupang@cluster0.x3m6qzb.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')

# Pilih database dan koleksi
db = client['DavDatabase']
collection = db['data_anime']

# Ambil data dari koleksi MongoDB
data_from_db = list(collection.find())

# Konversi data ke DataFrame pandas
df = pd.DataFrame(data_from_db)

# Simpan DataFrame sebagai file CSV
df.to_csv('data_anime.csv', index=False)

# Informasi pengguna bahwa data telah disimpan
print("Data telah disimpan sebagai 'anime_rating.csv'")

# Ubah 'your_connection_string' dengan string koneksi MongoDB Anda
client = MongoClient('mongodb+srv://ricardodirkanderson:rikupang@cluster0.x3m6qzb.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')

# Pilih database dan koleksi
db = client['DavDatabase']
collection = db['data_user']

# Ambil data dari koleksi MongoDB
data_from_db = list(collection.find())

# Konversi data ke DataFrame pandas
df = pd.DataFrame(data_from_db)

# Simpan DataFrame sebagai file CSV
df.to_csv('data_user.csv', index=False)

# Informasi pengguna bahwa data telah disimpan
print("Data telah disimpan sebagai 'anime_rating.csv'")



# Load dataset

anime = pd.read_csv('/content/data_anime.csv')
ratings = pd.read_csv('/content/anime_ratings.csv')
users = pd.read_csv('/content/data_user.csv')

anime

anime.info()

"""### Ratings

Berikut ini adalah isi dari berkas `Ratings.csv`
"""

ratings

ratings.groupby('score').count()

rating_counter = ratings.groupby('score').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating anime yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah anime')
plt.bar(rating_counter.index, rating_counter['anime_id'])
plt.grid(True)
plt.show()

"""Pada visualisasi data di atas dapat diketahui bahwa data tidak seimbang dan banyak pengguna yang memberikan rating 0."""

ratings.info()

ratings.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))

"""### Users

Berikut ini adalah isi dari `Users.csv`
"""

users

users.info()

users.describe()

"""## Data Preparation

Sebelum dapat dilakukan pemodelan, maka data harus melalui tahap data preparation terlebih dahulu. Berikut adalah langkah-langkah yang dilakukan dalam data preparation.

### Handling Imbalanced Data

Sebelumnya telah diketahui bahwa data rating tidak seimbang, untuk itu pada tahap ini saya mencoba untuk menghapus data rating 0.
"""

ratings.drop(ratings[ratings["score"] == 0].index, inplace=True)

"""Berikut ini adalah jumlah data setelah di-drop"""

ratings.shape

ratings

rating_counter = ratings.groupby('score').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating score yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah score')
plt.bar(rating_counter.index, rating_counter['anime_id'])
plt.grid(True)
plt.show()

"""### Encoding Data

Encoding dilakukan untuk menyandikan `User-ID` dan `ISBN` ke dalam indeks integer
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = ratings['user_id'].unique().tolist()

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah anime_id menjadi list tanpa nilai yang sama
anime_id_list = ratings['anime_id'].unique().tolist()

# Melakukan encoding anime_id
anime_id_to_anime_id_encoded = {x: i for i, x in enumerate(anime_id_list)}

# Melakukan proses encoding angka ke anime_id
anime_id_encoded_to_anime_id = {i: x for i, x in enumerate(anime_id_list)}

"""Setelah itu hasil dari encoding akan dimapping ke dataframe `ratings`"""

# Mapping userID ke dataframe user
ratings['user'] = ratings['user_id'].map(user_to_user_encoded)

# Mapping userID ke dataframe user
ratings['anime'] = ratings['anime_id'].map(anime_id_to_anime_id_encoded)

ratings

ratings.info()

"""### Randomize Dataset

Berikut ini adalah proses pengacakan data agar distribusi datanya menjadi random.
"""

# Mengacak dataset
df = ratings.sample(frac=1, random_state=42)
df

"""### Data Standardization and Splitting

Setelah datanya diacak, kemudian dataset dibagi menjadi 2 bagian, yaitu data yang akan digunakan untuk melatih model (sebesar 80%) dan data untuk memvalidasi model (sebesar 20%).

Selain itu juga dilakukan standarisasi nilai rating yang sebelumnya berada di rentang 0 hingga 10 kini diubah ke rentang 0 hingga 1 untuk mempermudah dalam proses training
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah
num_anime = len(anime_id_encoded_to_anime_id)
print(num_anime)

# Mengubah rating menjadi nilai float
df['score'] = df['score'].values.astype(np.float32)

# Nilai minimum score
min_rating = min(df['score'])

# Nilai maksimal score
max_rating = max(df['score'])

print('Number of User: {}, Number of anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

from sklearn.model_selection import train_test_split
# Tentukan nilai minimal dan maksimal dari rating
min_rating = df['score'].min()
max_rating = df['score'].max()

# Membuat variabel x untuk mencocokkan data user dan anime menjadi satu value
x = df[['user', 'anime']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['score'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

print(x_train, y_train)
print(x_val, y_val)

"""## Modelling

### Membuat Kelas RecommenderNet
"""

import tensorflow as tf
from tensorflow.keras import layers, regularizers, callbacks

class RecommenderNet(tf.keras.Model):

    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_anime = num_anime
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.anime_embedding = layers.Embedding(
            num_anime,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.anime_bias = layers.Embedding(num_anime, 1)
        self.dropout = layers.Dropout(0.5)
        self.batch_norm = layers.BatchNormalization()

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        anime_vector = self.anime_embedding(inputs[:, 1])
        anime_bias = self.anime_bias(inputs[:, 1])

        user_vector = self.batch_norm(user_vector)
        anime_vector = self.batch_norm(anime_vector)

        dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

        x = dot_user_anime + user_bias + anime_bias
        x = self.dropout(x)

        return tf.nn.sigmoid(x)

"""### Hyperparameter Tuning

Agar mendapatkan hasil model yang optimal, maka dalam proyek ini menggunakan bantuan library `optuna` untuk melakukan hyperparameter tuning atau pencarian nilai hyperparameter yang terbaik, dalam hal ini adalah nilai `embedding_size`.
"""

def objective(trial):
    tf.keras.backend.clear_session()
    model = RecommenderNet(num_users=num_users, num_anime=num_anime, embedding_size=trial.suggest_int('embedding_size', 1, 15))

    # model compile
    model.compile(
        loss = tf.keras.losses.BinaryCrossentropy(),
        optimizer = keras.optimizers.Adam(learning_rate=0.001),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    model.fit(
        x = x_train,
        y = y_train,
        batch_size=200,
        epochs = 1,
        validation_data = (x_val, y_val)
    )

    y_pred= model.predict(x_val)

    return mean_squared_error(y_val, y_pred, squared=False)

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=15, timeout=500)

print("Number of finished trials: {}".format(len(study.trials)))

print("Best trial:")
trial = study.best_trial

print("  Value: {}".format(trial.value))

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

tf.keras.backend.clear_session()

# Menerapkan nilai parameter paling optimal dari optuna
BEST_EMBEDDING_SIZE = 12

model = RecommenderNet(num_users, num_anime, BEST_EMBEDDING_SIZE)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""### Melatih Model"""

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Definisikan callback untuk early stopping
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Definisikan callback untuk menyimpan model terbaik
model_checkpoint = ModelCheckpoint(
    'best_model',  # Format default adalah 'tf'
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=True  # Hanya menyimpan bobot model
)

# Memulai training
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=50,  # Tambahkan lebih banyak epochs untuk memanfaatkan early stopping
    validation_data=(x_val, y_val),
    callbacks=[early_stopping, model_checkpoint]
)

"""## Evaluasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.grid(True)
plt.show()

"""Berdasarkan metriks tersebut menunjukkan bahwa model yang telah dibuat memiliki nilai Root Mean Squared Error (RMSE) sebesar 0.185

## Mendapatkan Rekomendasi
"""

anime_df = anime
df = pd.read_csv('/content/anime_ratings.csv')

anime_df

df

import numpy as np

# Mengambil sample user
user_id = 66126
anime_read_by_user = df[df['user_id'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
anime_not_read = anime_df[~anime_df['_id'].isin(anime_read_by_user.anime_id.values)]['_id']
anime_not_read = list(
    set(anime_not_read)
    .intersection(set(anime_id_to_anime_id_encoded.keys()))
)

anime_not_read = [[anime_id_to_anime_id_encoded.get(x)] for x in anime_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_read), anime_not_read)
)

# Pastikan user_anime_array memiliki bentuk yang benar untuk prediksi
print(f"user_anime_array shape: {user_anime_array.shape}")

ratings = model.predict(user_anime_array).flatten()

# Debug point: Periksa beberapa prediksi
print(f"Some predicted ratings: {ratings[:10]}")

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime = [
    anime_id_encoded_to_anime_id.get(anime_not_read[x][0]) for x in top_ratings_indices
]

# Debug point: Periksa anime yang direkomendasikan
print(f"Recommended anime IDs: {recommended_anime}")

# Menampilkan rekomendasi untuk pengguna
print('Menampilkan rekomendasi untuk pengguna: {}'.format(user_id))
print('===' * 9)

# Anime dengan rating tinggi dari pengguna
print('Anime dengan rating tinggi dari pengguna')
print('----' * 8)

# Get top anime rated by the user
top_anime_user = (
    anime_read_by_user.sort_values(
        by='score',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

# Select rows from anime_df corresponding to top_anime_user
anime_df_rows = anime_df[anime_df['_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.title, "-", row.score)

print('----' * 8)
print('Top 10 rekomendasi anime')
print('----' * 8)

# Get top 10 recommended anime
recommended_animes = anime_df[anime_df['_id'].isin(recommended_anime)]
for row in recommended_animes.itertuples():
    print(row.title, "-", row.score)

model.save_weights('best_model_cbl')